---
title: "Unknown perpetrator shootings analysis"
author: "Michele Lotto"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Requirements

```{r requirements}
requirements=c("summarytools", "pROC", "glmnetUtils", "dplyr", "car", "effects", "gridExtra", "grid", "MASS","e1071", "mgcv")

for (req in requirements){
  if (!require(req, character.only = TRUE)){
      install.packages(req)
  }
}
```

```{r, include=FALSE}
st_options(headings = FALSE)
```


# Loading data

```{r}
shootings <- readRDS("data/shootings_unknown.Rda")
print(dfSummary(shootings), method="render")
```


# Train/Test split

```{r}
set.seed(2)

train <- sample(nrow(shootings), 0.80*nrow(shootings))
test <- (-train)

shootings.train <- subset(shootings[train,], select = -murder )
shootings.test <- subset(shootings[test,], select = -murder )

dim(shootings.train)
dim(shootings.test)
```

# Logistic regression

Let's start with a model with all the predictors.

```{r}
glm.full <- glm(murder_prob ~ ., data=shootings.train, family=binomial)
summary(glm.full)
```

First check multicollinearity:

```{r}
vif(glm.full)
```

As we can see "city_location" predictors has an extremely large VIF and it is also not significant, we should remove it.

```{r}
glm.full <- update(glm.full, .~.-city_location)
vif(glm.full)
```

Now we check for influential points:

```{r}
influenceIndexPlot(glm.full, vars = "C")
```

The cook's distance plot does not give indication of influential points.

Now let's create a model with only significant predictors:

```{r}
glm.sig <- glm(murder_prob ~ day_period + year + week_day + COVID_pandemic + vic_age + vic_sex + other_victims + jurisdiction, data=shootings.train, family=binomial)
summary(glm.sig)
```

```{r}
AIC(glm.sig, glm.full)
```

The AIC indicates that `glm.sig` is better than the full model. Also the estimate coefficients are more interpretable.

Now we check for influential points:

```{r}
influenceIndexPlot(glm.sig, vars = "C")
```

The cook's distance plot does not give indication of influential points.

## Interaction terms

Now let's take a look at some interactions. In particular we add to `glm.sig` the following interactions:

- victim sex and victim age
- victim sex and victim race
- victim age and victim race
- year and day of the year

```{r}
glm.sign.inter <- update(glm.sig, . ~ . + vic_sex:vic_age + vic_sex:vic_race + vic_age:vic_race +year*day_year)
summary(glm.sign.inter)
```

As we can see the interaction between victim predictors are not significant, while the interaction terms between year and day of the year is significant. Let's remove the not significant interaction:

```{r}
glm.sign.inter <- update(glm.sign.inter, . ~ . - vic_sex:vic_age - vic_sex:vic_race - vic_age:vic_race)
summary(glm.sign.inter)
```

```{r}
AIC(glm.sign.inter, glm.sig)
```

The AIC decreases: we prefer the model with interaction.

Now let's try adding interaction terms to the full model. Now let's add the following interactions:

- victim sex and victim age
- victim sex and victim race
- victim age and victim race
- year and day of the yea

```{r}
glm.full.inter <- update(glm.full, . ~ . + vic_sex:vic_age + vic_sex:vic_race + vic_age:vic_race +year*day_year)
summary(glm.full.inter)
```

As before the interaction between victim predictors are not significant. Instead the interaction between year and day of the year is significant. Let's remove the not significant interaction:

```{r}
glm.full.inter <- update(glm.full.inter, . ~ . - vic_sex:vic_age - vic_sex:vic_race - vic_age:vic_race)
summary(glm.full.inter)
```

Check AIC:

```{r}
AIC(glm.full)
AIC(glm.full.inter)
```

The AIC decrease suggests that the full model with interaction is better compared to the full model.

In summary the models with interaction are preferable. Now let's compare the two models with interaction:

```{r}
AIC(glm.full.inter)
AIC(glm.sign.inter)
```

In terms of AIC we prefer the full model with interaction.

Let's interpret what the best model in terms of AIC tell us.

```{r}
summary(glm.full.inter)
```

Let's plot the effects for all the interactionless variables:

```{r, fig.height=5, fig.width=15}
eff <- head(allEffects(glm.full.inter),-1)

plot(head(eff,2))
```
As we can see:

- during the late hours of the day it is more likely for the victim to survive a shooting incident.

- during the weekend it is more likely for the victim to survive a shooting incident.

```{r, fig.height=12, fig.width=18}
eff <- head(allEffects(glm.full.inter),-1)
eff[1] <- NULL
eff[1] <- NULL
eff[8] <- NULL

plot(eff)
```

As we can see:

- COVID lockdown effect is not significant as the confidence bands tells us.

- it is less likely for a victim to survive during COVID pandemic period.

- the working hour effect is not significant as the confidence bands tells us.

- as Latitude increases the more likely for the victim to survive, while Longitude is not significant as the confidence bands tells us.

- the more old the victim is the less likely for him/her is to survive.

- males victims are less likely to survive.

- as the number of additional victims grows the less likely for the victim is to survive.

- if the jurisdiction of the shooting incident is patrol it is less likely for the victim to survive.


```{r}
plot(effect("vic_race", glm.full.inter))
```
As we can see if the victim is "BLACK HISPANIC" it is more likely for him/her to survive.

Now let's plot interaction:

```{r, fig.height=5, fig.width=15}
eff <- allEffects(glm.full.inter)[length(allEffects(glm.full.inter))]
plot(eff)
```

As we can see as the years grows the effect of the day of the year on murder probability decreases and from negative correlated becomes positive correlated with murder probability.

## Model comparison

Let's compare all the models done so far in terms of prediction power to be able to compare them with other types of models:

```{r}
glm.full.pred <- predict(glm.full, newdata = shootings.test, type = "response")

glm.sig.pred <- predict(glm.sig, newdata = shootings.test, type = "response")

glm.full.inter.pred <- predict(glm.full.inter, newdata = shootings.test, type = "response")

glm.sig.inter.pred <- predict(glm.sign.inter, newdata = shootings.test, type = "response")
```

Select the best threshold via ROC curve:

```{r}
par(mfrow=c(2,2))

glm.full.roc <- roc(shootings.test$murder_prob ~ glm.full.pred, plot=TRUE, print.auc=TRUE, main="glm.full ROC curve")
glm.sig.roc <- roc(shootings.test$murder_prob ~ glm.sig.pred, plot=TRUE, print.auc=TRUE, main="glm.sig ROC curve")

glm.full.inter.roc <- roc(shootings.test$murder_prob ~ glm.full.inter.pred, plot=TRUE, print.auc=TRUE, main="glm.full.inter ROC curve")
glm.sig.inter.roc <- roc(shootings.test$murder_prob ~ glm.sig.inter.pred, plot=TRUE, print.auc=TRUE, main="glm.sig.inter ROC curve")
```

As we can see, the ROC curve is essentially the same for all the models except for `glm.sig.inter`.

```{r}
glm.full.metrics <- coords(glm.full.roc, x="best", ret="all")
glm.sig.metrics <- coords(glm.sig.roc, x="best", ret="all")
glm.full.inter.metrics <- coords(glm.full.inter.roc, x="best", ret="all")
glm.sig.inter.metrics <- coords(glm.sig.inter.roc, x="best", ret="all")
```

```{r}
row.names(glm.full.metrics) <- "Full model"
row.names(glm.sig.metrics) <- "Significant predictors model"
row.names(glm.full.inter.metrics) <- "Full model with interaction"
row.names(glm.sig.inter.metrics) <- "Significant predictors model with interaction"

metrics <- rbind(glm.full.metrics, glm.sig.metrics, glm.full.inter.metrics, glm.sig.inter.metrics)
```

Now let's compare:

1) specificity
```{r}
(metrics %>% arrange(desc(specificity)))[, c("specificity", "sensitivity", "accuracy")]
```

The best model in terms of specificity is "Full model with interaction".

2) sensitivity
```{r}
(metrics %>% arrange(desc(sensitivity)))[, c("specificity", "sensitivity", "accuracy")]
```

The best model in terms of sensitivity is "Full model".

3) accuracy
```{r}
(metrics %>% arrange(desc(accuracy)))[, c("specificity", "sensitivity", "accuracy")]
```

The best model in terms of accuracy is "Full model with interaction".

Since the response in unbalanced:

```{r}
print(dfSummary(shootings.test[,"murder_prob"]), method="render")
```

The best model in terms of prediction power should maximize its ability to correctly classify murders. For this reason i should choose the model with higher sensitivity and acceptable specificity (>0.5): "Full model".

#### Confusion Matrixes

```{r}
confusion_matrix <- function(pred_prob, threshold){
  pred_class <- pred_prob > threshold
  table(preds=pred_class, true=as.logical(shootings.test$murder_prob))
}
```

1) Full model

```{r}
glm.full.metrics[, c("specificity", "sensitivity", "accuracy")]
```


```{r}
confusion_matrix(glm.full.pred, glm.full.metrics$threshold)
```

2) Full model with interaction

```{r}
glm.full.inter.metrics[, c("specificity", "sensitivity", "accuracy")]
```


```{r}
confusion_matrix(glm.full.inter.pred, glm.full.inter.metrics$threshold)
```

3) Significant predictors model

```{r}
glm.sig.metrics[, c("specificity", "sensitivity", "accuracy")]
```


```{r}
confusion_matrix(glm.sig.pred, glm.sig.metrics$threshold)
```

4) Significant predictors model with interaction

```{r}
glm.sig.inter.metrics[, c("specificity", "sensitivity", "accuracy")]
```


```{r}
confusion_matrix(glm.sig.inter.pred, glm.sig.inter.metrics$threshold)
```

## Model selection

Now let's apply automatic model selection to identity the best model.

### Stepwiese logistic regression

We use the model which has the most predictors and interaction terms as starting point.

```{r}
model <- glm(murder_prob ~ . + vic_sex:vic_age + vic_sex:vic_race + vic_age:vic_race +year*day_year, family = binomial, data = shootings.train)
glm.step <- step(model, direction = "both", trace = FALSE)
```

```{r}
summary(glm.step)
```

```{r}
AIC(glm.full, glm.full.inter, glm.sig, glm.sign.inter, glm.step) %>% arrange(AIC)
```

In terms of AIC the step model is the best.

#### Model comparison

Let's compare the step model with the models done so far.

```{r}
glm.step.pred <- predict(glm.step, newdata = shootings.test, type = "response")
```

```{r}
glm.step.roc <- roc(shootings.test$murder_prob ~ glm.step.pred, plot=TRUE, print.auc=TRUE, main="Step model ROC curve")
```

```{r}
glm.step.metrics <- coords(glm.step.roc, x="best", ret="all")
rownames(glm.step.metrics) <- "Step model"
```

```{r}
metrics <- rbind(metrics, glm.step.metrics)
```

```{r}
(metrics %>% arrange(desc(sensitivity)))[, c("specificity", "sensitivity", "accuracy")]
```

The step model is the worst in terms of sensitivity but has good specificity and accuracy.

Confusion matrix for step model:

```{r}
confusion_matrix(glm.step.pred, glm.step.metrics$threshold)
```


### Lasso Regression

Now we try with lasso regression. As before we use as starting point the formula with all predictors and interactions term.

```{r}
lasso.mod <- glmnet(murder_prob ~ . + vic_sex:vic_age + vic_sex:vic_race + vic_age:vic_race +year*day_year, data=shootings.train, alpha=1, family = "binomial")
plot(lasso.mod, main='Path plot of the Lasso estimates\n\n')
```

We choose lambda using cross validation:

```{r}
lasso.cv <- cv.glmnet(murder_prob ~ . + vic_sex:vic_age + vic_sex:vic_race + vic_age:vic_race +year*day_year, data=shootings.train, alpha=1, family = "binomial")
plot(lasso.cv)
```

Lasso gives us the choice between two values of lambda:

- `lambda.min`: lambda of minimum mean cross-validated error.

- `lambda.1se`: largest value of lambda such that error is within 1 standard error of the cross-validated errors for `lambda.min`.

We explore both paths.

#### 1se lambda

```{r}
plot(lasso.mod, xvar = "lambda", main='Path plot of the Lasso estimates with 1se lambda\n\n')
abline(v = log(lasso.cv$lambda.1se), lty="dashed")
```

The remaining coefficients are:

```{r}
lasso.1se.coef <- coef(lasso.mod,  s=lasso.cv$lambda.1se)

lasso.1se.coef[lasso.1se.coef[,1]!=0,]
```

#### min lambda

```{r}
plot(lasso.mod, xvar = "lambda", main='Path plot of the Lasso estimates with min lambda\n\n')
abline(v =log(lasso.cv$lambda.min), lty="dashed")
```

The remaining coefficients are:

```{r}
lasso.min.coef <- coef(lasso.mod,  s=lasso.cv$lambda.min)
lasso.min.coef[lasso.min.coef[,1]!=0,]
```

#### Model comparison

Now let's compare prediction performance of lasso models.

```{r}
lasso.1se.pred <- predict(lasso.mod, s=lasso.cv$lambda.1se, newdata = shootings.test, type = "response")
lasso.min.pred <- predict(lasso.mod, s=lasso.cv$lambda.min, newdata = shootings.test, type = "response")
```

```{r, fig.width=10, fig.height=5}
par(mfrow=c(1,2))

lasso.1se.roc <- roc(shootings.test$murder_prob ~ lasso.1se.pred, plot=TRUE, print.auc=TRUE, main="Lasso 1se lambda ROC curve")
lasso.min.roc <- roc(shootings.test$murder_prob ~ lasso.min.pred, plot=TRUE, print.auc=TRUE, main="Lasso min lambda ROC curve")
```
According to the ROC curve the Lasso model with min lambda is better than the one with 1se lambda.

```{r}
lasso.1se.metrics <- coords(lasso.1se.roc, x="best", ret="all")
row.names(lasso.1se.metrics) <- "Lasso 1se lambda"

lasso.min.metrics <- coords(lasso.min.roc, x="best", ret="all")
row.names(lasso.min.metrics) <- "Lasso min lambda"
```

```{r}
metrics <- rbind(metrics, lasso.1se.metrics, lasso.min.metrics)
```

```{r}
(metrics %>% arrange(desc(sensitivity)))[, c("specificity", "sensitivity", "accuracy")]
```

As we can see "Lasso min lambda" is the best model in terms of sensitivity now. The model "Lasso 1se lambda" is the second best model in terms of sensitivity now. Thus the best overall model for now is "Lasso min lambda"

Confusion Matrix for `lambda.1se`:

```{r}
confusion_matrix(lasso.1se.pred, lasso.1se.metrics$threshold)
```

Confusion Matrix for `lambda.min`:

```{r}
confusion_matrix(lasso.min.pred, lasso.min.metrics$threshold)
```

### Ridge logistic regression

Now we try with ridge regression. As before we use as starting point the formula with all predictors and interaction term.

```{r}
ridge.mod <- glmnet(murder_prob ~ . + vic_sex:vic_age + vic_sex:vic_race + vic_age:vic_race +year*day_year, data=shootings.train, alpha=0, family = binomial)
plot(ridge.mod, main='Path plot of the Ridge estimates\n\n')
```

We choose lambda using cross validation:

```{r}
ridge.cv <- cv.glmnet(murder_prob ~ . + vic_sex:vic_age + vic_sex:vic_race + vic_age:vic_race +year*day_year, data=shootings.train, alpha=0, family = binomial)
plot(ridge.cv)
```

Ridge (as Lasso) gives us the choice between two values of lambda:

- `lambda.min`: lambda of minimum mean cross-validated error.

- `lambda.1se`: largest value of lambda such that error is within 1 standard error of the cross-validated errors for `lambda.min`.

We explore both paths.

#### 1se lambda

```{r}
plot(ridge.mod, xvar = "lambda", main='Path plot of the Ridge estimates with 1se lambda\n\n')
abline(v =log(ridge.cv$lambda.1se), lty="dashed")
```

```{r}
ridge.1se.coef <- coef(ridge.mod,  s=ridge.cv$lambda.1se)
ridge.1se.coef
```

Differently from Lasso, ridge doesn't force to zero the coefficient thus is difficult to interpret those numbers.

#### min lambda

```{r}
plot(ridge.mod, xvar = "lambda", main='Path plot of the Ridge estimates with min lambda\n\n')
abline(v =log(ridge.cv$lambda.min), lty="dashed")
```

```{r}
ridge.min.coef <- coef(ridge.mod,  s=ridge.cv$lambda.min)
ridge.min.coef
```

#### Model comparison

```{r}
ridge.1se.pred <- predict(ridge.mod, s=ridge.cv$lambda.1se, newdata = shootings.test, type = "response")
ridge.min.pred <- predict(ridge.mod, s=ridge.cv$lambda.min, newdata = shootings.test, type = "response")
```

```{r, fig.width=10, fig.height=5}
par(mfrow=c(1,2))

ridge.1se.roc <- roc(shootings.test$murder_prob ~ ridge.1se.pred, plot=TRUE, print.auc=TRUE, main="1se lambda Ridge ROC curve")
ridge.min.roc <- roc(shootings.test$murder_prob ~ ridge.min.pred, plot=TRUE, print.auc=TRUE, main="min lambda Ridge ROC curve")
```

```{r}
ridge.1se.metrics <- coords(ridge.1se.roc, x="best", ret="all")
row.names(ridge.1se.metrics) <- "ridge 1se lambda"

ridge.min.metrics <- coords(ridge.min.roc, x="best", ret="all")
row.names(ridge.min.metrics) <- "ridge min lambda"
```

```{r}
metrics <- rbind(metrics, ridge.1se.metrics, ridge.min.metrics)
```

```{r}
(metrics %>% arrange(desc(sensitivity)))[, c("specificity", "sensitivity", "accuracy")]
```

Ridge 1se lambda model is the best in terms of sensitivity. Ridge min lambda is above average in terms of sensitivity. Thus the best overall model is Ridge 1se lambda.

Confusion Matrix for `lambda.1se`:

```{r}
confusion_matrix(ridge.1se.pred, ridge.1se.metrics$threshold)
```

Confusion Matrix for `lambda.min`:

```{r}
confusion_matrix(ridge.min.pred, ridge.min.metrics$threshold)
```

# Naive Bayes

```{r}
nb.fit <- naiveBayes(formula(glm.full), data = shootings.train)
nb.fit
```

```{r}
nb.pred <- predict(nb.fit, newdata = shootings.test, type = "raw")
```

```{r}
nb.roc <- roc(shootings.test$murder_prob ~ nb.pred[,2], plot=TRUE, print.auc=TRUE, main="Naive Bayes ROC curve")
```

```{r}
nb.metrics <- coords(nb.roc, x="best", ret="all")
row.names(nb.metrics) <- "Naive bayes"
```

```{r}
metrics <- rbind(metrics, nb.metrics)
```

```{r}
(metrics %>% arrange(desc(sensitivity)))[, c("specificity", "sensitivity", "accuracy")]
```

Naive Bayes is below average in terms of sensitivity.

Confusion matrix for Naive Bayes:

```{r}
confusion_matrix(nb.pred[,2], nb.metrics$threshold)
```

# GAM

Now let's fit a GAM model with all predictors (interaction term included). I applied splines to all the numerical predictors.

```{r}

f <- update(formula(glm.full.inter), . ~ . -Latitude -Longitude -day_year -year -year:day_year + s(year) + s(day_year, bs="cc")+ s(Latitude) + s(Longitude))
gam.fit <- gam(f, data=shootings.train, family=binomial)
 
summary(gam.fit)
```

Plot of the splines effect:

```{r, fig.height=10, fig.width=12}
par(mfrow=c(2,3))

for (i in 1:5){
  plot(gam.fit, select=i, shade=TRUE, shade.col = "lightblue")
  abline(h=0, lty="dashed")
}
```

As we can see both for the model summary and the plots the effect of Longitude seams linear; day of the year seams periodic and year and Latitude highly non linear.

Now let's try fitting a GAM model now with only significant predictors:

```{r}
gam2.fit <- update(gam.fit, . ~ . -s(Longitude) +Longitude -COVID_lockdown -COVID_pandemic -working_hour -vic_race)
 
summary(gam2.fit)
```

```{r, fig.height=5, fig.width=12}
par(mfrow=c(1,3))

for (i in 1:5){
  plot(gam2.fit, select=i, shade=TRUE, shade.col = "lightblue")
  abline(h=0, lty="dashed")
}
```
The splines effects doesn't change much compared to the previews model.

## Model comparison

```{r}
gam.predict <- predict(gam.fit, newdata = shootings.test, type = "response")
gam2.predict <- predict(gam2.fit, newdata = shootings.test, type = "response")
```

```{r, fig.width=10, fig.height=5}
par(mfrow=c(1,2))

gam.roc <- roc(shootings.test$murder_prob ~ gam.predict, plot=TRUE, print.auc=TRUE, main="GAM 1 ROC curve")
gam2.roc <- roc(shootings.test$murder_prob ~ gam2.predict, plot=TRUE, print.auc=TRUE, main="GAM 2 ROC curve")
```

The ROC curves are quite similar.

```{r}
gam.metrics <- coords(gam.roc, x="best", ret="all")
gam2.metrics <- coords(gam2.roc, x="best", ret="all")

row.names(gam.metrics) <- "GAM1"
row.names(gam2.metrics) <- "GAM2"
```

```{r}
metrics <- rbind(metrics, gam.metrics, gam2.metrics)
```

```{r}
(metrics %>% arrange(desc(sensitivity)))[, c("specificity", "sensitivity", "accuracy")]
```

"GAM2" model is the worst in terms of sensitivity but gives the best results in accuracy and specificity; while "GAM1" is the second worst model in terms of sensitivity.

# Conclusions

Let's find the best model for all the metrics:

1) specificity
```{r}
(metrics %>% arrange(desc(specificity)))[, c("specificity", "sensitivity", "accuracy")]
```

The best model in terms of specificity is "Full model with interaction".

2) sensitivity
```{r}
(metrics %>% arrange(desc(sensitivity)))[, c("specificity", "sensitivity", "accuracy")]
```

The best model in terms of sensitivity is "Ridge 1se lambda".

3) accuracy
```{r}
(metrics %>% arrange(desc(accuracy)))[, c("specificity", "sensitivity", "accuracy")]
```

The best model in terms of accuracy is "Full model with interaction".

Since the response in unbalanced:

```{r}
print(dfSummary(shootings.test[,"murder_prob"]), method="render")
```

The best model in terms of prediction power should maximize its ability to correctly classify murders. For this reason i should choose the model with higher sensitivity and acceptable specificity (>0.5): "Ridge 1se lambda".

Here are its coefficients:

```{r}
coef(ridge.mod, s=ridge.cv$lambda.1se)
```

```{r}
metrics["ridge 1se lambda", c("specificity", "sensitivity", "accuracy")]
```

Confusion matrix:

```{r}
confusion_matrix(glm.step.pred, glm.step.metrics$threshold)
```
